{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from model import Generator\n",
    "from dataset import MultiResolutionDataset\n",
    "import moviepy.editor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_label = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,), inplace=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(256, 512, 8, 2, architecture='spade').cuda()\n",
    "ckpt = torch.load('checkpoint_256_spade_with_noise/250000.pt')\n",
    "generator.load_state_dict(ckpt['g_ema'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_w(label, w):\n",
    "    with torch.no_grad():\n",
    "        res = generator(label.unsqueeze(0).cuda(), [w], input_is_latent=True)\n",
    "\n",
    "    res = res[0].cpu()[0].numpy()\n",
    "    res = np.transpose(res, (1, 2, 0))\n",
    "    res = (res * 0.5 + 0.5) * 255\n",
    "    res = np.clip(res, 0, 255).astype(np.uint8)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = len(glob('youtube_512_one_person/label*'))\n",
    "paths = [f'youtube_512_one_person/label_{i}.jpg' for i in range(n_labels)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = {}\n",
    "fps = 30\n",
    "\n",
    "for mp3_filename in [f for f in os.listdir('audio_data') if f.endswith('.mp3')]:\n",
    "    mp3_filename = f'audio_data/{mp3_filename}'\n",
    "    wav_filename = mp3_filename[:-4] + '.wav'\n",
    "    if not os.path.exists(wav_filename):\n",
    "        audio_clip = moviepy.editor.AudioFileClip(mp3_filename)\n",
    "        audio_clip.write_audiofile(wav_filename, fps=44100, nbytes=2, codec='pcm_s16le')\n",
    "    track_name = os.path.basename(wav_filename)[15:-5]\n",
    "    rate, signal = wavfile.read(wav_filename)\n",
    "    signal = np.mean(signal, axis=1) # to mono\n",
    "    signal = np.abs(signal)\n",
    "    duration = signal.shape[0] / rate\n",
    "    frames = int(np.ceil(duration * fps))\n",
    "    samples_per_frame = signal.shape[0] / frames\n",
    "    audio[track_name] = np.zeros(frames, dtype=signal.dtype)\n",
    "    for frame in range(frames):\n",
    "        start = int(round(frame * samples_per_frame))\n",
    "        stop = int(round((frame + 1) * samples_per_frame))\n",
    "        audio[track_name][frame] = np.mean(signal[start:stop], axis=0)\n",
    "    audio[track_name] /= max(audio[track_name])\n",
    "\n",
    "for track in sorted(audio.keys()):\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    plt.title(track)\n",
    "    plt.plot(audio[track])\n",
    "    plt.savefig(f'audio_data/{track}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ws(n, frames, seed):\n",
    "    filename = f'audio_data/ws_{n}_{frames}_{seed}.npy'\n",
    "    if not os.path.exists(filename):\n",
    "        src_ws = np.random.RandomState(seed).randn(n, 512)\n",
    "        ws = np.empty((frames, 512))\n",
    "        for i in range(512):\n",
    "            # FIXME: retarded\n",
    "            x = np.linspace(0, 3*frames, 3*len(src_ws), endpoint=False)\n",
    "            y = np.tile(src_ws[:, i], 3)\n",
    "            x_ = np.linspace(0, 3*frames, 3*frames, endpoint=False)\n",
    "            y_ = interp1d(x, y, kind='quadratic', fill_value='extrapolate')(x_)\n",
    "            ws[:, i] = y_[frames:2*frames]\n",
    "        np.save(filename, ws)\n",
    "    else:\n",
    "        ws = np.load(filename)\n",
    "    return ws\n",
    "\n",
    "def mix_styles(wa, wb, ivs):\n",
    "    w = np.copy(wa)\n",
    "    for i, v in ivs:\n",
    "        w[i] = wa[i] * (1 - v) + wb[i] * v\n",
    "    return w\n",
    "\n",
    "def normalize_vector(v):\n",
    "    return v * np.std(w_avg) / np.std(v) + np.mean(w_avg) - np.mean(v)\n",
    "\n",
    "def render_frame(t):\n",
    "    global base_index\n",
    "    frame = np.clip(np.int(np.round(t * fps)), 0, frames - 1)\n",
    "    base_index += base_speed * audio[''][frame]**2\n",
    "    base_w = base_ws[int(round(base_index)) % len(base_ws)]\n",
    "    \n",
    "    psi = 0.7\n",
    "    base_w = w_avg + (base_w - w_avg) * psi\n",
    "    w = base_w\n",
    "    w += mouth_open * audio[''][frame] * 0.5\n",
    "    \n",
    "    label_frame = np.clip(np.int(np.round(len(paths) / frames * frame)), 0, len(paths) - 1)\n",
    "    label = transform_label(Image.open(paths[label_frame]))\n",
    "    image = generate_with_w(label, torch.tensor(w, dtype=torch.float32).cuda())\n",
    "    image = Image.fromarray(image).resize((size, size), PIL.Image.LANCZOS)\n",
    "    return np.array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(16384, 512, device='cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    w = generator.style(z)\n",
    "\n",
    "w_avg = w.mean(0, keepdim=True).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 512\n",
    "seconds = int(np.ceil(duration))\n",
    "resolution = 10\n",
    "base_frames = resolution * frames\n",
    "base_ws = get_ws(seconds, base_frames, seed)\n",
    "base_speed = base_frames / sum(audio['']**2)\n",
    "base_index = 0\n",
    "mix_ws = get_ws(seconds, frames, seed + 1)\n",
    "mouth_open = normalize_vector(np.random.RandomState(seed + 2).randn(512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp4_filename = 'audio_data/dua_lipa.mp4'\n",
    "video_clip = moviepy.editor.VideoClip(render_frame, duration=duration)\n",
    "audio_clip_i = moviepy.editor.AudioFileClip('audio_data/dua_lipa.wav')\n",
    "# audio_clip_v = moviepy.editor.AudioFileClip('data/Culture Shock (Vocal).wav')\n",
    "audio_clip = moviepy.editor.CompositeAudioClip([audio_clip_i])\n",
    "video_clip = video_clip.set_audio(audio_clip)\n",
    "video_clip.write_videofile(mp4_filename, fps=fps, codec='libx264', audio_codec='aac', bitrate='8M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
